package agent

import (
	"context"
	"encoding/json"
	"fmt"
	"qwq/internal/config"
	"qwq/internal/utils"
	"regexp"
	"strings"
	"time"

	openai "github.com/sashabaranov/go-openai"
)

const (
	DefaultModel   = "Qwen/Qwen2.5-7B-Instruct"
	DefaultBaseURL = "https://api.siliconflow.cn/v1"
)

var Client *openai.Client

func InitClient() {
	cfg := openai.DefaultConfig(config.GlobalConfig.ApiKey)
	if config.GlobalConfig.BaseURL != "" {
		cfg.BaseURL = config.GlobalConfig.BaseURL
	} else {
		cfg.BaseURL = DefaultBaseURL
	}
	Client = openai.NewClientWithConfig(cfg)
}

var Tools = []openai.Tool{
	{
		Type: openai.ToolTypeFunction,
		Function: &openai.FunctionDefinition{
			Name:        "execute_shell_command",
			Description: "Execute a shell command on the local Linux/MacOS system.",
			Parameters: json.RawMessage(`{
				"type": "object",
				"properties": {
					"command": { "type": "string", "description": "The shell command" },
					"reason": { "type": "string", "description": "The reason" }
				},
				"required": ["command", "reason"]
			}`),
		},
	},
}

func GetBaseMessages() []openai.ChatCompletionMessage {
	knowledgePart := ""
	if config.CachedKnowledge != "" {
		knowledgePart = fmt.Sprintf("\nã€å†…éƒ¨çŸ¥è¯†åº“ã€‘:\n%s\n", config.CachedKnowledge)
	}

	sysPrompt := fmt.Sprintf(`ä½ æ˜¯ä¸€ä¸ª **Linux è¿ç»´ç»ˆç«¯**ã€‚
å½“å‰ç”¨æˆ·æ˜¯ **Root ç®¡ç†å‘˜**ã€‚

ã€è§„åˆ™ã€‘
1. **å¿…é¡»ä½¿ç”¨ä¸­æ–‡** å›žå¤ã€‚
2. å½“ç”¨æˆ·è¦æ±‚æŸ¥è¯¢æˆ–æ“ä½œæ—¶ï¼Œ**å¿…é¡»** è°ƒç”¨ execute_shell_command å·¥å…·ã€‚
3. å¦‚æžœæ— æ³•è°ƒç”¨å·¥å…·ï¼Œè¯·ç›´æŽ¥è¾“å‡º Shell å‘½ä»¤ï¼ŒåŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼Œä¾‹å¦‚ï¼š
   `+"```bash"+`
   free -m
   `+"```"+`
4. **ç¦æ­¢è§£é‡Š**ï¼šä¸è¦è¯´â€œä½ å¯ä»¥ä½¿ç”¨...â€ï¼Œç›´æŽ¥ç»™å‡ºç»“æžœã€‚

%s`, knowledgePart)

	return []openai.ChatCompletionMessage{
		{Role: openai.ChatMessageRoleSystem, Content: sysPrompt},
		// æ ·æœ¬ï¼šå¼ºåˆ¶å®ƒè®¤ä¸ºè‡ªå·±å·²ç»æ‰§è¡Œè¿‡å‘½ä»¤äº†
		{Role: openai.ChatMessageRoleUser, Content: "çœ‹çœ‹å†…å­˜"},
		{
			Role: openai.ChatMessageRoleAssistant,
			ToolCalls: []openai.ToolCall{{
				ID: "call_1", Type: openai.ToolTypeFunction,
				Function: openai.FunctionCall{Name: "execute_shell_command", Arguments: `{"command": "free -m", "reason": "check memory"}`},
			}},
		},
	}
}

func AnalyzeWithAI(issue string) string {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer cancel()

	msgs := GetBaseMessages()
	msgs = append(msgs, openai.ChatCompletionMessage{Role: openai.ChatMessageRoleUser, Content: issue})

	resp, err := Client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
		Model: getModelName(),
		Messages: msgs,
		Temperature: 0.1,
	})
	if err != nil {
		return "AI è¿žæŽ¥å¤±è´¥: " + err.Error()
	}
	return resp.Choices[0].Message.Content
}

func ProcessAgentStep(msgs *[]openai.ChatCompletionMessage) (openai.ChatCompletionMessage, bool) {
	return ProcessAgentStepForWeb(msgs, func(log string) {
		fmt.Println(log)
	})
}

func ProcessAgentStepForWeb(msgs *[]openai.ChatCompletionMessage, logCallback func(string)) (openai.ChatCompletionMessage, bool) {
	ctx := context.Background()
	reqCtx, cancel := context.WithTimeout(ctx, 5*time.Minute)
	defer cancel()
	
	resp, err := Client.CreateChatCompletion(reqCtx, openai.ChatCompletionRequest{
		Model: getModelName(),
		Messages: *msgs, 
		Tools: Tools, 
		Temperature: 0.1,
	})
	
	if err != nil {
		logCallback(fmt.Sprintf("API Error: %v", err))
		return openai.ChatCompletionMessage{}, false
	}
	msg := resp.Choices[0].Message
	*msgs = append(*msgs, msg)

	if len(msg.ToolCalls) > 0 {
		for _, toolCall := range msg.ToolCalls {
			handleToolCall(toolCall, msgs, logCallback)
		}
		return msg, true
	}

	cmd := extractCommandFromText(msg.Content)
	if cmd != "" {
		logCallback(fmt.Sprintf("âš¡ (è¡¥æ•‘æ¨¡å¼) è¯†åˆ«åˆ°å‘½ä»¤: %s", cmd))
		
		if !utils.IsCommandSafe(cmd) {
			logCallback("âŒ [æ‹¦æˆª] é«˜å±å‘½ä»¤")
			return msg, false
		}

		output := utils.ExecuteShell(cmd)
		if strings.TrimSpace(output) == "" { output = "(No output)" }
		
		feedback := fmt.Sprintf("[System Output]:\n%s", output)
		*msgs = append(*msgs, openai.ChatCompletionMessage{Role: openai.ChatMessageRoleUser, Content: feedback})
		
		return msg, true
	}

	return msg, true
}

func handleToolCall(toolCall openai.ToolCall, msgs *[]openai.ChatCompletionMessage, logCallback func(string)) {
	if toolCall.Function.Name == "execute_shell_command" {
		var args map[string]string
		json.Unmarshal([]byte(toolCall.Function.Arguments), &args)
		cmdStr := strings.TrimSpace(args["command"])
		reason := args["reason"]
		if cmdStr == "" { return }

		logCallback(fmt.Sprintf("âš¡ æ„å›¾: %s", reason))
		logCallback(fmt.Sprintf("ðŸ‘‰ å‘½ä»¤: %s", cmdStr))

		if !utils.IsCommandSafe(cmdStr) {
			logCallback("âŒ [æ‹¦æˆª] é«˜å±å‘½ä»¤")
			addToolOutput(msgs, toolCall.ID, "Error: Blocked.")
			return
		}

		if utils.IsReadOnlyCommand(cmdStr) {
			// Auto run
		} else {
			logCallback("âš ï¸ Webæ¨¡å¼æš‚ä¸æ”¯æŒäº¤äº’å¼ä¿®æ”¹å‘½ä»¤ï¼Œå·²è·³è¿‡")
			addToolOutput(msgs, toolCall.ID, "User denied.")
			return
		}

		output := utils.ExecuteShell(cmdStr)
		if strings.TrimSpace(output) == "" { output = "(No output)" }
		addToolOutput(msgs, toolCall.ID, output)
	}
}

func addToolOutput(msgs *[]openai.ChatCompletionMessage, id, content string) {
	*msgs = append(*msgs, openai.ChatCompletionMessage{Role: openai.ChatMessageRoleTool, Content: content, ToolCallID: id})
}

func getModelName() string {
	if config.GlobalConfig.Model != "" {
		return config.GlobalConfig.Model
	}
	return DefaultModel
}

func extractCommandFromText(text string) string {
	re := regexp.MustCompile("(?s)```(?:bash|shell|sh)?\\n(.*?)\\n```")
	matches := re.FindStringSubmatch(text)
	if len(matches) > 1 {
		return strings.TrimSpace(matches[1])
	}
	reSingle := regexp.MustCompile("`([^`]+)`")
	matchesSingle := reSingle.FindStringSubmatch(text)
	if len(matchesSingle) > 1 {
		cmd := matchesSingle[1]
		if strings.HasPrefix(cmd, "docker") || strings.HasPrefix(cmd, "free") || strings.HasPrefix(cmd, "df") || strings.HasPrefix(cmd, "top") || strings.HasPrefix(cmd, "uptime") {
			return cmd
		}
	}
	return ""
}