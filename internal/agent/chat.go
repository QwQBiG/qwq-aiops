package agent

import (
	"context"
	"encoding/json"
	"fmt"
	"qwq/internal/config"
	"qwq/internal/utils"
	"strings"
	"time"

	openai "github.com/sashabaranov/go-openai"
)

const (
	DefaultModel   = "Qwen/Qwen2.5-7B-Instruct"
	DefaultBaseURL = "https://api.siliconflow.cn/v1"
)

var Client *openai.Client

func InitClient() {
	// [ä¿®æ”¹] ä½¿ç”¨ GlobalConfig
	cfg := openai.DefaultConfig(config.GlobalConfig.ApiKey)
	cfg.BaseURL = DefaultBaseURL
	Client = openai.NewClientWithConfig(cfg)
}

var Tools = []openai.Tool{
	{
		Type: openai.ToolTypeFunction,
		Function: &openai.FunctionDefinition{
			Name:        "execute_shell_command",
			Description: "Execute a shell command on the local Linux/MacOS system.",
			Parameters: json.RawMessage(`{
				"type": "object",
				"properties": {
					"command": { "type": "string", "description": "The shell command" },
					"reason": { "type": "string", "description": "The reason (in Chinese)" }
				},
				"required": ["command", "reason"]
			}`),
		},
	},
}

func AnalyzeWithAI(issue string) string {
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	knowledgePart := ""
	if config.CachedKnowledge != "" {
		knowledgePart = fmt.Sprintf("\nã€å†…éƒ¨çŸ¥è¯†åº“ã€‘:\n%s\n", config.CachedKnowledge)
	}

	sysPrompt := fmt.Sprintf(`ä½ æ˜¯ä¸€ä¸ªç´§æ€¥æ•…éšœå“åº”ä¸“å®¶ã€‚
è§„åˆ™ï¼š
1. **æåº¦ç®€ç»ƒ**ï¼šåªè¾“å‡ºæ ¸å¿ƒåŸå› å’Œä¸€æ¡ä¿®å¤å‘½ä»¤ã€‚
2. **æ‹’ç»åºŸè¯**ï¼šä¸è¦è§£é‡ŠåŸç†ã€‚
3. **ç©ºæ•°æ®é˜²å¾¡**ï¼šå¦‚æœè¾“å…¥åªåŒ…å«è¡¨å¤´è€Œæ²¡æœ‰æ•°æ®ï¼Œå›ç­”â€œè¯¯æŠ¥â€ã€‚
4. **åƒµå°¸è¿›ç¨‹ç‰¹åˆ¤**ï¼šå¿…é¡»æ€æ‰çˆ¶è¿›ç¨‹(PPID)ã€‚
%s`, knowledgePart)

	resp, err := Client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
		Model: DefaultModel, Messages: []openai.ChatCompletionMessage{{Role: "system", Content: sysPrompt}, {Role: "user", Content: issue}}, Temperature: 0.1,
	})
	if err != nil {
		return "AI è¿æ¥å¤±è´¥: " + err.Error()
	}
	return resp.Choices[0].Message.Content
}

// ProcessAgentStep å¤„ç† Chat æ¨¡å¼çš„å•æ­¥é€»è¾‘
func ProcessAgentStep(msgs *[]openai.ChatCompletionMessage) (openai.ChatCompletionMessage, bool) {
	ctx := context.Background()
	fmt.Print("\033[33mğŸ¤– æ€è€ƒä¸­...\033[0m\r")
	resp, err := Client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{Model: DefaultModel, Messages: *msgs, Tools: Tools, Temperature: 0.1})
	if err != nil {
		fmt.Printf("\nAPI Error: %v\n", err)
		return openai.ChatCompletionMessage{}, false
	}
	msg := resp.Choices[0].Message
	*msgs = append(*msgs, msg)

	if len(msg.ToolCalls) > 0 {
		for _, toolCall := range msg.ToolCalls {
			if toolCall.Function.Name == "execute_shell_command" {
				var args map[string]string
				json.Unmarshal([]byte(toolCall.Function.Arguments), &args)
				cmdStr := strings.TrimSpace(args["command"])
				reason := args["reason"]
				if cmdStr == "" { continue }

				fmt.Printf("\n\033[36mâš¡ æ„å›¾: %s\033[0m\n", reason)
				fmt.Printf("\033[33mğŸ‘‰ å‘½ä»¤: \033[1m%s\033[0m\n", cmdStr)

				if !utils.IsCommandSafe(cmdStr) {
					fmt.Println("\033[31m[æ‹¦æˆª] é«˜å±å‘½ä»¤\033[0m")
					addToolOutput(msgs, toolCall.ID, "Error: Blocked.")
					continue
				}

				if utils.IsReadOnlyCommand(cmdStr) {
					fmt.Println("\033[90m(è‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å‘½ä»¤...)\033[0m")
				} else {
					if !utils.ConfirmExecution() {
						fmt.Println("\033[90må·²è·³è¿‡\033[0m")
						addToolOutput(msgs, toolCall.ID, "User denied.")
						continue
					}
				}

				fmt.Print("\033[90mæ‰§è¡Œä¸­...\033[0m")
				output := utils.ExecuteShell(cmdStr)
				if strings.TrimSpace(output) == "" { output = "(No output)" }
				fmt.Printf("\r\033[32mâœ” å®Œæˆ\033[0m\n")
				addToolOutput(msgs, toolCall.ID, output)
			}
		}
		return msg, true
	}
	return msg, true
}

func addToolOutput(msgs *[]openai.ChatCompletionMessage, id, content string) {
	*msgs = append(*msgs, openai.ChatCompletionMessage{Role: openai.ChatMessageRoleTool, Content: content, ToolCallID: id})
}